{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.74  Python-3.10.16 torch-2.6.0+cpu CPU (12th Gen Intel Core(TM) i7-12650H)\n",
      "Setup complete  (16 CPUs, 15.7 GB RAM, 17.2/60.0 GB disk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\AI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_det_model_path = 'runs/detect/train/weights/best.pt'\n",
    "yolo = YOLO(text_det_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\AI\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--timm--resnet101.a1h_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers, dropout=0.2, unfreeze_layers=3):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        backbone = timm.create_model(\n",
    "            'resnet101',\n",
    "            in_chans=1,\n",
    "            pretrained=True\n",
    "        )\n",
    "        modules = list(backbone.children())[:-2]\n",
    "        modules.append(nn.AdaptiveAvgPool2d((1, None)))\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "\n",
    "        # Unfreeze the last few layers\n",
    "        for parameter in self.backbone[-unfreeze_layers:].parameters():\n",
    "            parameter.requires_grad = True\n",
    "\n",
    "        self.mapSeq = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            512, hidden_size,\n",
    "            n_layers, bidirectional=True, batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, vocab_size),\n",
    "            nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # Flatten the feature map\n",
    "        x = self.mapSeq(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.out(x)\n",
    "        x = x.permute(1, 0, 2)  # Based on CTC\n",
    "\n",
    "        return x\n",
    "\n",
    "chars = '0123456789abcdefghijklmnopqrstuvwxyz-'\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(chars))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "hidden_size = 256\n",
    "n_layers = 3\n",
    "dropout_prob = 0.2\n",
    "unfreeze_layers = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_path = 'ocr_crnn.pt'\n",
    "\n",
    "print(\"Hello World\")\n",
    "\n",
    "crnn_model = CRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout_prob,\n",
    "    unfreeze_layers=unfreeze_layers\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoded_sequences, idx_to_char, blank_char='-'):\n",
    "    decoded_sequences = []\n",
    "    \n",
    "    for seq in encoded_sequences:\n",
    "        decoded_label = []\n",
    "        for idx, token in enumerate(seq):\n",
    "            if token != 0:\n",
    "                char = idx_to_char[token.item()]\n",
    "                if char != blank_char:\n",
    "                    decoded_label.append(char)\n",
    "        \n",
    "        decoded_sequences.append(''.join(decoded_label))\n",
    "    \n",
    "    return decoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_detection(img_path, text_det_model):\n",
    "    text_det_results = text_det_model(img_path, verbose=False)[0]\n",
    "    \n",
    "    bboxes = text_det_results.boxes.xyxy.tolist()\n",
    "    classes = text_det_results.boxes.cls.tolist()\n",
    "    names = text_det_results.names\n",
    "    confs = text_det_results.boxes.conf.tolist()\n",
    "    \n",
    "    return bboxes, classes, names, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_recognition(img, data_transforms, text_reg_model, idx_to_char, device):\n",
    "    transformed_image = data_transforms(img)\n",
    "    transformed_image = transformed_image.unsqueeze(0).to(device)\n",
    "    text_reg_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = text_reg_model(transformed_image).detach().cpu()\n",
    "        text = decode(logits.permute(1, 0, 2).argmax(2), idx_to_char)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(img, detections):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for bbox, detected_class, confidence, transcribed_text in detections:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2))\n",
    "        plt.text(\n",
    "            x1, y1 - 10, f\"{detected_class} ({confidence:.2f}): {transcribed_text}\",\n",
    "            fontsize=9, bbox=dict(facecolor='red', alpha=0.5)\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((100, 420)),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.GaussianBlur(3),\n",
    "        transforms.RandomAffine(degrees=1, shear=1),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3, interpolation=3),\n",
    "        transforms.RandomRotation(degrees=2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((100, 420)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path, data_transforms, text_det_model, text_reg_model, idx_to_char, device):\n",
    "    # Detection\n",
    "    bboxes, classes, names, confs = text_detection(img_path, text_det_model)\n",
    "    \n",
    "    # Load the image\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Iterate through the results\n",
    "    for bbox, cls, conf in zip(bboxes, classes, confs):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        confidence = conf\n",
    "        detected_class = cls\n",
    "        name = names[int(cls)]\n",
    "        \n",
    "        # Extract the detected object and crop it\n",
    "        cropped_image = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        transcribed_text = text_recognition(\n",
    "            cropped_image,\n",
    "            data_transforms['val'],\n",
    "            text_reg_model,\n",
    "            idx_to_char,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        predictions.append((bbox, name, confidence, transcribed_text))\n",
    "    \n",
    "    visualize_detections(img, predictions)\n",
    "    \n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
